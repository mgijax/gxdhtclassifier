## June 1, 2021
Created gxdhtclassifier repo in github
Started work on sdGetKnownSamples.py to pull training/test sets from the db.
Need to figure out:
    how to select known "yes"s and "no"s
        Connie's _user_key = 1064
        GXD HT ArrayExpress Load _user_key = 1561
            1257 "no"s have this as "evaluated by"
        the rest, "yes"s and "no"s and a few "maybe"s are "evaluated by" Connie
            9506 no
            3186 yes
              13 maybe
    Initially:
        only take the "connie" ones
        only take GEO experiments. (those w/ GEO IDs)
        Use evaluation_state to define "yes" or "no".
            Don't look at curaton status
    Need to validate these decisions with Connie

select t.term, u._user_key, count(*)
from gxd_htexperiment e join voc_term t on (e._evaluationstate_key = t._term_key)
join mgi_user u on (e._evaluatedby_key = u._user_key)
group by t.term, u._user_key
||
select u.name, count(*)
from gxd_htexperiment e join mgi_user u on (e._evaluatedby_key = u._user_key)
group by u._user_key
||
select t.term, u.name, count(*)
from gxd_htexperiment e join voc_term t on (e._evaluationstate_key = t._term_key)
join mgi_user u on (e._evaluatedby_key = u._user_key)
join acc_accession a on (a._object_key = e._experiment_key and a._mgitype_key = 42 and a._logicaldb_key = 190)
group by t.term, u.name

## June 8, 2021
Initial sdGetKnownSamples.py results
    Result counts:
    Tue Jun  8 13:50:41 2021
    Hitting database bhmgidevdb01.jax.org prod as mgd_public
    GEO experiments evaluated by Connie
       2624 Yes	   7305 No	   9929 total
    Total time:    1.092 seconds

## June 12, 2021
Initial version of htMLsample.py - module defines Sample classes for HT
experiments: HtSample and ClassifiedHtSample.
Used this in sdGetKnownSamples.py to create an initial sample set.

Modified baseSampleDataLib.py in MLtextTools to terminate the #meta lines of
sample with a '\n' (always) instead of using the recordEnd string that is
specified in the Sample class. This removed an annoying circularity in needing
to know the Sample class name to be able to read the #meta line that contained
the Sample class name. This came out of wanting to use '\n' to end HtSample
records since they are generally short.

Looking at the initial sample set (same as above):
    Mon Jun 14 16:30:21 2021
    Hitting database mgi-adhoc.jax.org mgd as mgd_public
    GEO experiments evaluated by Connie
       2624 Yes	   7305 No	   9929 total
This is roughly 26% / 74% split. Not TOO badly balanced.
All the records seem to have reasonable length titles and descriptions, except
one record, GSE15354, has a null description, and that seems to match its 
record in GEO.

One idea: add additional non-GEO "yes" experiments to balance it better.
    There are 562 of these experiments evaluated by Connie.
    I'll include these in the sample set. This gets us to about 30% / 70%

## June 15, 2021
Have a complete version of sdGetKnownSamples.py
It outputs two sample sets: 
    all GEO evaluated by Connie
    all non-GEO "Yes" experiments evaluted by Connie
    Tue Jun 15 17:12:19 2021
Output counts: 
    Hitting database mgi-adhoc.jax.org mgd as mgd_public
    GEO experiments evaluated by Connie
       2624 (26%) Yes	   7305 (73%) No	   9929 total
    Non-GEO, Yes experiments evaluated by Connie
        562 experiments
    Total experiments
       3186 (30%) Yes	   7305 (69%) No	  10491 total
Want Connie to review this data set and thinking to complete YAKS-21

## July 9, 2021
Completed splitSamples.py in MLtextTools. Generic script for randomly
splitting sample files for creating validation and testing sets.

## July 14, 2021
created sdBuild1Get.sh and sdBuild4Split.sh - wrapper scripts to get sample
sets from the database and split out training, validation, and test sets.
These use mgiconfig and a Configuration file.

## July 23, 2021
Added functionality in MLtextTools/tuningReportsLib.py getBestParamsReport()
to print output of get_params() method. This dumps the Pipeline steps params
better. (on yaks branch)

Did some tuning of random forest classifier. Overfitting some, mostly played
with min_samples_split to try to correct overfitting.
Here is the best run:

### Start Time 2021/07/23-10-17-46  RF.py       index file: index.out
Random Seeds:   randForClassifier=939   randForSplit=706   
### Metrics: Training Set
              precision    recall  f1-score   support

   Train Yes       0.84      0.88      0.86      2263
    Train No       0.94      0.92      0.93      4726
Train (Yes) F2: 0.8730    P: 0.8404    R: 0.8816    NPV: 0.9419

['Yes', 'No']
[[1995  268]
 [ 379 4347]]

### Metrics: Validation Set
              precision    recall  f1-score   support

   Valid Yes       0.69      0.75      0.72       528
    Valid No       0.91      0.88      0.89      1477
Valid (Yes) F2: 0.7383    P: 0.6947    R: 0.7500    NPV: 0.9080

['Yes', 'No']
[[ 396  132]
 [ 174 1303]]

### Best Pipeline Parameters:
classifier__min_samples_split: 100
classifier__n_estimators: 100
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 2)

This run uses 822 features. No stemming, but remove stop words

Will switch to doing some feature preprocessing: feature transformations.

## July 27, 2021
Still using RF.
Still overfitting some, would need to work harder to shrink this.
In general, seems to do (slightly) better with:
    binary vectorization (not counts)
    (1,2) ngrams (better than (1,1) or (1,3))
    Note: vectorizer removes stop words prior to n-gramming in all cases.
    100 estimators (tried 50, 125, 150)

Did some feature preprocessing, helped a tiny bit.
Feature transformations (tumor types, embryonic day, KO, WT, ...)

With no preprocessing:
### Start Time 2021/07/23-16-46-06  RF.py
Train (Yes) F2: 0.8730    P: 0.8404    R: 0.8816    NPV: 0.9419
Valid (Yes) F2: 0.7383    P: 0.6947    R: 0.7500    NPV: 0.9080

With feature transforms:
### Start Time 2021/07/27-10-12-12  RF.py
Train (Yes) F2: 0.8701    P: 0.8481    R: 0.8758    NPV: 0.9396
Valid (Yes) F2: 0.7436    P: 0.7057    R: 0.7538    NPV: 0.9098

With feature transforms and stemming:
### Start Time 2021/07/27-10-46-34  RF.py
Train (Yes) F2: 0.8686    P: 0.8423    R: 0.8754    NPV: 0.9392
Valid (Yes) F2: 0.7557    P: 0.7005    R: 0.7708    NPV: 0.9150

Note the NPV is pretty good (with and w/o any preprocessing).
Why is recall so much lower?
Because the validation set is a little unbalanced, so there are fewer positives,
and therefore fewer predicted true positives compared the number of false
negatives.

TODO: 1) should review feature transformationsn with Connie and see if they make
sense to her or if she has other ideas for tranformations.
2) should try gradient boosted forest, maybe SGD
