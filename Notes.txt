## June 1, 2021
Created gxdhtclassifier repo in github
Started work on sdGetKnownSamples.py to pull training/test sets from the db.
Need to figure out:
    how to select known "yes"s and "no"s
        Connie's _user_key = 1064
        GXD HT ArrayExpress Load _user_key = 1561
            1257 "no"s have this as "evaluated by"
        the rest, "yes"s and "no"s and a few "maybe"s are "evaluated by" Connie
            9506 no
            3186 yes
              13 maybe
    Initially:
        only take the "connie" ones
        only take GEO experiments. (those w/ GEO IDs)
        Use evaluation_state to define "yes" or "no".
            Don't look at curaton status
    Need to validate these decisions with Connie

select t.term, u._user_key, count(*)
from gxd_htexperiment e join voc_term t on (e._evaluationstate_key = t._term_key)
join mgi_user u on (e._evaluatedby_key = u._user_key)
group by t.term, u._user_key
||
select u.name, count(*)
from gxd_htexperiment e join mgi_user u on (e._evaluatedby_key = u._user_key)
group by u._user_key
||
select t.term, u.name, count(*)
from gxd_htexperiment e join voc_term t on (e._evaluationstate_key = t._term_key)
join mgi_user u on (e._evaluatedby_key = u._user_key)
join acc_accession a on (a._object_key = e._experiment_key and a._mgitype_key = 42 and a._logicaldb_key = 190)
group by t.term, u.name

## June 8, 2021
Initial sdGetKnownSamples.py results
    Result counts:
    Tue Jun  8 13:50:41 2021
    Hitting database bhmgidevdb01.jax.org prod as mgd_public
    GEO experiments evaluated by Connie
       2624 Yes	   7305 No	   9929 total
    Total time:    1.092 seconds

## June 12, 2021
Initial version of htMLsample.py - module defines Sample classes for HT
experiments: HtSample and ClassifiedHtSample.
Used this in sdGetKnownSamples.py to create an initial sample set.

Modified baseSampleDataLib.py in MLtextTools to terminate the #meta lines of
sample with a '\n' (always) instead of using the recordEnd string that is
specified in the Sample class. This removed an annoying circularity in needing
to know the Sample class name to be able to read the #meta line that contained
the Sample class name. This came out of wanting to use '\n' to end HtSample
records since they are generally short.

Looking at the initial sample set (same as above):
    Mon Jun 14 16:30:21 2021
    Hitting database mgi-adhoc.jax.org mgd as mgd_public
    GEO experiments evaluated by Connie
       2624 Yes	   7305 No	   9929 total
This is roughly 26% / 74% split. Not TOO badly balanced.
All the records seem to have reasonable length titles and descriptions, except
one record, GSE15354, has a null description, and that seems to match its 
record in GEO.

One idea: add additional non-GEO "yes" experiments to balance it better.
    There are 562 of these experiments evaluated by Connie.
    I'll include these in the sample set. This gets us to about 30% / 70%

## June 15, 2021
Have a complete version of sdGetKnownSamples.py
It outputs two sample sets: 
    all GEO evaluated by Connie
    all non-GEO "Yes" experiments evaluted by Connie
    Tue Jun 15 17:12:19 2021
Output counts: 
    Hitting database mgi-adhoc.jax.org mgd as mgd_public
    GEO experiments evaluated by Connie
       2624 (26%) Yes	   7305 (73%) No	   9929 total
    Non-GEO, Yes experiments evaluated by Connie
        562 experiments
    Total experiments
       3186 (30%) Yes	   7305 (69%) No	  10491 total
Want Connie to review this data set and thinking to complete YAKS-21

## July 9, 2021
Completed splitSamples.py in MLtextTools. Generic script for randomly
splitting sample files for creating validation and testing sets.

## July 14, 2021
created sdBuild1Get.sh and sdBuild4Split.sh - wrapper scripts to get sample
sets from the database and split out training, validation, and test sets.
These use mgiconfig and a Configuration file.

## July 23, 2021
Added functionality in MLtextTools/tuningReportsLib.py getBestParamsReport()
to print output of get_params() method. This dumps the Pipeline steps params
better. (on yaks branch)

Did some tuning of random forest classifier. Overfitting some, mostly played
with min_samples_split to try to correct overfitting.
Here is the best run:

### Start Time 2021/07/23-10-17-46  RF.py       index file: index.out
Random Seeds:   randForClassifier=939   randForSplit=706   
### Metrics: Training Set
              precision    recall  f1-score   support

   Train Yes       0.84      0.88      0.86      2263
    Train No       0.94      0.92      0.93      4726
Train (Yes) F2: 0.8730    P: 0.8404    R: 0.8816    NPV: 0.9419

['Yes', 'No']
[[1995  268]
 [ 379 4347]]

### Metrics: Validation Set
              precision    recall  f1-score   support

   Valid Yes       0.69      0.75      0.72       528
    Valid No       0.91      0.88      0.89      1477
Valid (Yes) F2: 0.7383    P: 0.6947    R: 0.7500    NPV: 0.9080

['Yes', 'No']
[[ 396  132]
 [ 174 1303]]

### Best Pipeline Parameters:
classifier__min_samples_split: 100
classifier__n_estimators: 100
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 2)

This run uses 822 features. No stemming, but remove stop words

Will switch to doing some feature preprocessing: feature transformations.

## July 27, 2021
Still using RF.
Still overfitting some, would need to work harder to shrink this.
In general, seems to do (slightly) better with:
    binary vectorization (not counts)
    (1,2) ngrams (better than (1,1) or (1,3))
    Note: vectorizer removes stop words prior to n-gramming in all cases.
    100 estimators (tried 50, 125, 150)

Did some feature preprocessing, helped a tiny bit.
Feature transformations (tumor types, embryonic day, KO, WT, ...)

With no preprocessing:
### Start Time 2021/07/23-16-46-06  RF.py
Train (Yes) F2: 0.8730    P: 0.8404    R: 0.8816    NPV: 0.9419
Valid (Yes) F2: 0.7383    P: 0.6947    R: 0.7500    NPV: 0.9080

With feature transforms:
### Start Time 2021/07/27-10-12-12  RF.py
Train (Yes) F2: 0.8701    P: 0.8481    R: 0.8758    NPV: 0.9396
Valid (Yes) F2: 0.7436    P: 0.7057    R: 0.7538    NPV: 0.9098

With feature transforms and stemming:
### Start Time 2021/07/27-10-46-34  RF.py
Train (Yes) F2: 0.8686    P: 0.8423    R: 0.8754    NPV: 0.9392
Valid (Yes) F2: 0.7557    P: 0.7005    R: 0.7708    NPV: 0.9150

Note the NPV is pretty good (with and w/o any preprocessing).
Why is recall so much lower?
Because the validation set is a little unbalanced, so there are fewer positives,
and therefore fewer predicted true positives compared the number of false
negatives.

TODO: 1) should review feature transformationsn with Connie and see if they make
sense to her or if she has other ideas for tranformations.
2) should try gradient boosted forest, maybe SGD

## Aug 13, 2021
Created TextTransformer and TextMapping classes in MLtextTools/utilsLib.py.
Provides transform reports (what mappings were applied) duing preprocessing.
Modified gxdhtclassifier/htFeatureTransform.py and htMLsample.py to start using
the new classes. This helped debug some of the mappings. In particular
early_embryo mapping was matching way too many things.

## Aug 18, 2021
Added TextMappingFromStrings and TextMappingFromFile subclasses to build 
mappings from piles of strings. Working on applying this to cell lines listed
in PRB_cellline table. BUT there are lots of terms in the table that we cannot
map to __cell_line, e.g., RNA, E1, F2, "Not Specified", so we are going to
need to do some manual cleanup.
Need to:
    1) compare Debbies tumor cell line list w/ PRB_celllines
        Done, see compareCellLines.txt
        Summary: Deb's list has 178, 64 found in PRB_cellline, 114 not found
    2) do simple, first pass clean up the PRB_cellline names
        Done. Commented out some (#). Will need to get Connie to look this
        over better and evaluate results from (1)
    3) Will map both to __cell_line, but the mappings will be distinct
        Done.
        3.a) play with relaxing matching of " ", "-", "/" in debbie's names?
    4) create a text mapping for tumor types.
        Done.
    5) create full transformation report on corpus (Done. see Sep 7 below)
    6) preprocess the data sets and rebuild models and compare results.
        Done. See below
    7) clean up automated tests: utilsLib.py (new subclasses),
        htFeatureTransform.py (new mappings)

Questions for Connie
    1) what concept distinctions make sense? tumor type vs. cell line?
        Should tumor type become "disease"? Try to match other diseases?
        Genotypes: is distinction between +/+  -/- important?
            Should it just be "_genotype" (include the words like "genotypes?,
            mutants?" ?
    2) there is a lot of junk in PRB_cellline. How best to clean it up?
        Compare to Debbie's cell line list. See comparison report.
    3) Done. (for jim mostly) think about how to fix embryonic day mappings
    4) do we keep "KO" for knockout - how often is the mapping right vs. wrong
        See Sep 7 (6) below.

## Sep 7, 2021
Getting back to this.
    1) adjusted _embryonic_day mapping to skip E1, E2, E3 and digit
        combinations that don't make sense (e.g., E26)
        This looks much better.
    2) added (early|mid|late) streak | morula | somites? to __embryo_age
        mappings
    3) added dpc, Theiler stages to __embryo_age
    4) added __mouse_age mapping: postnatal, newborn, etc.
    5) added __escell and __mef mappings
    6) reported context on KO mappings since 'KO' matched so many times, but
        these look reasonable as often are <gene name> KO, etc.
## Sep 9, 2021
    7) generated full transformation report on trainSet, valSet, testSet files
        for Connie and Richard to review.
        https://docs.google.com/spreadsheets/d/1Y2gowqqLSNuodMEG1qx5JWDUagqa-F_VwNxteV5JkKQ/edit#gid=219520192

    8) reran RF classifier on training/validation set using all the updated 
        transformations.
        Helped a bit - in particular Precision:
        Older transformations (and stemming)
        2021/07/28-10-22-25     F2PRNPV 0.7549  0.7036  0.7689  0.9146  RF.py
        2021/07/28-11-55-47     F2PRNPV 0.7589  0.7083  0.7727  0.9160  RF.py

        Newer transformations (and stemming)
        2021/09/10-11-03-05     F2PRNPV 0.7586  0.7199  0.7689  0.9153  RF.py
        2021/09/10-11-04-13     F2PRNPV 0.7448  0.7179  0.7519  0.9098  RF.py
        2021/09/10-11-04-53     F2PRNPV 0.7603  0.7276  0.7689  0.9157  RF.py

## Sep 15, 2021
    meeting to review this work w/ Connie and others:
    https://docs.google.com/presentation/d/1gMxIBpb9ukpRUWIx1C0z5798QaHxYwPrtnZfJZumvJ0/edit#slide=id.p

## Sep 28, 2021
Implemented Connie’s culled cell line list above.
Merged __embryo_age into __mouse_age.
Merged __wildtype and __mutant (added hetero/homo zygous)
Added __treat for various flavors of treated/treatment

Results: slight increase in NPV and R, slight decrease of P (based on averaging a few training runs)

Before changes:
2021/09/10-11-03-05     F2PRNPV 0.7586  0.7199  0.7689  0.9153  RF.py
2021/09/10-11-04-13     F2PRNPV 0.7448  0.7179  0.7519  0.9098  RF.py
2021/09/10-11-04-53     F2PRNPV 0.7603  0.7276  0.7689  0.9157  RF.py

After all changes:
2021/09/28-15-02-25     F2PRNPV 0.7659  0.7198  0.7784  0.9184  RF.py
2021/09/28-15-03-04     F2PRNPV 0.7555  0.7061  0.7689  0.9147  RF.py
2021/09/28-15-04-15     F2PRNPV 0.7601  0.7133  0.7727  0.9163  RF.py

matches from the sample set are here: 
https://docs.google.com/spreadsheets/d/1Y2gowqqLSNuodMEG1qx5JWDUagqa-F_VwNxteV5JkKQ/edit#gid=515131090

Feature weights from the training set are here:
https://docs.google.com/spreadsheets/d/1Y2gowqqLSNuodMEG1qx5JWDUagqa-F_VwNxteV5JkKQ/edit#gid=1717776770

Calling this experimentation DONE and going to move on to adding raw sample metadata text to the experiment titles and descriptions.
